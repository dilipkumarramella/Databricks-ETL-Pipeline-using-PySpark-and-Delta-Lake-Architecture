{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf61b38b-d345-42c3-ab0e-562ba6a77170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Databricks ETL Pipeline using PySpark and Delta Lake Architecture\n",
    "\n",
    "####Introduction\n",
    "\n",
    "This project implements a complete ETL pipeline on Azure Databricks using PySpark and the Delta Lake architecture. The goal is to ingest raw datasets from Kaggle using API, store them in Azure Data Lake Storage Gen2 (Bronze layer), process and transform the data using PySpark, and organize the cleaned and enriched data into Silver, and Gold layers following the medallion architecture.\n",
    "\n",
    "##### Key Steps in the Workflow:\n",
    "\n",
    "**Data Ingestion** - Download datasets from Kaggle using the Kaggle API.\n",
    "\n",
    "**Data Access Setup** – Configure OAuth-based connection between Databricks and ADLS.\n",
    "\n",
    "**Load into Bronze Layer** – Store the raw data files in ADLS (Bronze folder).\n",
    "\n",
    "**Data Transformation** – Use PySpark to clean, filter, aggregate, and enrich the data.\n",
    "\n",
    "**Silver & Gold Layers** – Save processed data back to ADLS in Silver (cleaned) and Gold (business-ready) folders using Delta format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff47073-0044-4920-9e6b-6e4e17e7d51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Data Ingestion\n",
    "\n",
    "**Step 1**: Install the Kaggle CLI package and restart Python to apply changes.<br>\n",
    "**Step 2**: Upload the kaggle.json file (downloadable from your Kaggle API account) to the DBFS Filestore and verify the upload by listing the directory.<br>\n",
    "**Step 3**: Ensure the JSON file is readable, then set the Kaggle configuration directory using the KAGGLE_CONFIG_DIR environment variable.<br>\n",
    "**Step 4**: Download the required dataset using the Kaggle CLI and unzip the contents for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bdd4cf-0aa2-4b40-85ee-e8f73dfe2674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: kaggle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a757848e-a8c8-4d53-8bf2-890c7862eede/lib/python3.10/site-packages (1.7.4.5)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.10/site-packages (from kaggle) (4.1.0)\nRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a757848e-a8c8-4d53-8bf2-890c7862eede/lib/python3.10/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.10/site-packages (from kaggle) (3.19.4)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.28.1)\nRequirement already satisfied: python-dateutil>=2.5.3 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: text-unidecode in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a757848e-a8c8-4d53-8bf2-890c7862eede/lib/python3.10/site-packages (from kaggle) (1.3)\nRequirement already satisfied: setuptools>=21.0.0 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (63.4.1)\nRequirement already satisfied: urllib3>=1.15.1 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (1.26.11)\nRequirement already satisfied: charset-normalizer in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.0.4)\nRequirement already satisfied: certifi>=14.05.14 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2022.9.14)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.10/site-packages (from kaggle) (0.5.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-a757848e-a8c8-4d53-8bf2-890c7862eede/lib/python3.10/site-packages (from kaggle) (4.67.1)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from kaggle) (3.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from bleach->kaggle) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->bleach->kaggle) (3.0.9)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc248766-65fd-42f0-9374-d7123506fb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a7b16d-2b1c-4c78-bfa3-7da4c2e64c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/tables/kaggle/kaggle.json', name='kaggle.json', size=73, modificationTime=1755096345000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/kaggle/movies-dataset-for-feature-extracion-prediction.zip', name='movies-dataset-for-feature-extracion-prediction.zip', size=1102493, modificationTime=1755096656000)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if JSON file is present\n",
    "dbutils.fs.ls('/FileStore/tables/kaggle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c91fa05-299b-468e-887d-95d9863a8968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\":\"dilipkumarramella\",\"key\":\"cc2cc967a2e3be857473bdc279429594\"}"
     ]
    }
   ],
   "source": [
    "# To check if the file is readable\n",
    "!cat /dbfs/FileStore/tables/kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71dbbc81-6440-48bc-ad66-13812e5f556d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/dbfs/FileStore/tables/kaggle/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb0bef4-85a2-4c47-983d-5dea2b794f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /dbfs/FileStore/tables/kaggle/kaggle.json'\r\nDataset URL: https://www.kaggle.com/datasets/bharatnatrayn/movies-dataset-for-feature-extracion-prediction\r\nLicense(s): CC0-1.0\r\nDownloading movies-dataset-for-feature-extracion-prediction.zip to /dbfs/FileStore/tables/kaggle\r\n\r  0%|                                               | 0.00/1.05M [00:00<?, ?B/s]\r\n\r100%|██████████████████████████████████████| 1.05M/1.05M [00:00<00:00, 12.0MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d bharatnatrayn/movies-dataset-for-feature-extracion-prediction -p /dbfs/FileStore/tables/kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97d978d-e994-41a4-8f6e-f632e4f60321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = \"/dbfs/FileStore/tables/kaggle/movies-dataset-for-feature-extracion-prediction.zip\"\n",
    "extract_path = \"/dbfs/FileStore/tables/kaggle/movies_dataset\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57aba826-2d1d-4ad4-a651-17e13ab685d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Data Access Setup  \n",
    "\n",
    "**Step 1:** Register a Service Principal in Azure AD and generate a client secret. Note down the Application (client) ID and Tenant ID.  \n",
    "**Step 2:** Grant the Service Principal access to your ADLS account by assigning the *Storage Blob Data Contributor* role.  \n",
    "**Step 3:** Store the client secret securely in Azure Key Vault.  \n",
    "**Step 4:** Create an Azure Key Vault-backed secret scope in Databricks and link it to your workspace.  \n",
    "**Step 5:** Configure OAuth in Databricks using the secret scope and Spark configuration settings to enable secure ADLS access.  \n",
    "\n",
    "Reference: [Connect to Azure Data Lake Storage – Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/connect/storage/tutorial-azure-storage)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5242c571-3809-45e8-b7a0-dc2413e4d4bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure OAuth for ADLS and Databricks Connection"
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"myscope\",key=\"ADLStoDatabricks\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.deltaetlstorage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.deltaetlstorage.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.deltaetlstorage.dfs.core.windows.net\", \"96151c83-cb41-427c-9983-db06a024c0eb\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.deltaetlstorage.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.deltaetlstorage.dfs.core.windows.net\", \"https://login.microsoftonline.com/0e46b9cd-f9de-44e6-8720-1e792845704b/oauth2/token/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d811b5-07ae-422f-8b2d-ffb088bfe6a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check Connection to ADLS Storage Account"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ADLS\n"
     ]
    }
   ],
   "source": [
    "# To check the connectivity\n",
    "container_name = \"delta-lakehouse\"\n",
    "storage_account = \"deltaetlstorage\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/\")\n",
    "    print(\"Connected to ADLS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to ADLS:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "954bff6a-4d2e-46d2-a97c-f931a963eead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Mount ADLS to Databricks Using OAuth Configuration\n",
    "\n",
    "#####Why Mount After Testing the Connection\n",
    "I first connected to ADLS using OAuth to confirm that the service principal, permissions, and networking were all set up correctly. This step helped isolate any authentication or access issues before moving forward.\n",
    "\n",
    "After confirming connectivity, I mounted the ADLS container to Databricks so I could work with it using short, clean paths like /mnt/ADLSmount instead of long URLs. Mounting also makes it easier to reuse the same storage location across multiple notebooks and jobs without re-entering the OAuth config every time.\n",
    "\n",
    "Reference: [Mounting cloud object storage on Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/dbfs/mounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3d075c-91bf-47d7-8170-959934394c04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mount ADLS to Databricks Using OAuth Configuration"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"96151c83-cb41-427c-9983-db06a024c0eb\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"myscope\",key=\"ADLStoDatabricks\"),\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/0e46b9cd-f9de-44e6-8720-1e792845704b/oauth2/token\"\n",
    "}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://delta-lakehouse@deltaetlstorage.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/ADLSmount\",\n",
    "    extra_configs = configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883e4b34-69ea-4136-89e4-c254d99d7746",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Mounted Filesystems in Databricks"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mountPoint</th><th>source</th><th>encryptionType</th></tr></thead><tbody><tr><td>/databricks-datasets</td><td>databricks-datasets</td><td></td></tr><tr><td>/Volumes</td><td>UnityCatalogVolumes</td><td></td></tr><tr><td>/databricks/mlflow-tracking</td><td>databricks/mlflow-tracking</td><td></td></tr><tr><td>/databricks-results</td><td>databricks-results</td><td></td></tr><tr><td>/databricks/mlflow-registry</td><td>databricks/mlflow-registry</td><td></td></tr><tr><td>/mnt/ADLSmount</td><td>abfss://delta-lakehouse@deltaetlstorage.dfs.core.windows.net/</td><td></td></tr><tr><td>/Volume</td><td>DbfsReserved</td><td></td></tr><tr><td>/volumes</td><td>DbfsReserved</td><td></td></tr><tr><td>/</td><td>DatabricksRoot</td><td></td></tr><tr><td>/volume</td><td>DbfsReserved</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "/databricks-datasets",
         "databricks-datasets",
         ""
        ],
        [
         "/Volumes",
         "UnityCatalogVolumes",
         ""
        ],
        [
         "/databricks/mlflow-tracking",
         "databricks/mlflow-tracking",
         ""
        ],
        [
         "/databricks-results",
         "databricks-results",
         ""
        ],
        [
         "/databricks/mlflow-registry",
         "databricks/mlflow-registry",
         ""
        ],
        [
         "/mnt/ADLSmount",
         "abfss://delta-lakehouse@deltaetlstorage.dfs.core.windows.net/",
         ""
        ],
        [
         "/Volume",
         "DbfsReserved",
         ""
        ],
        [
         "/volumes",
         "DbfsReserved",
         ""
        ],
        [
         "/",
         "DatabricksRoot",
         ""
        ],
        [
         "/volume",
         "DbfsReserved",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mountPoint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "encryptionType",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5470b593-f73d-408b-95ab-20c23dea946d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df35ff7c-279c-4058-a8db-6745149a1877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad668878-5c69-4f2a-a850-45bdefa1648f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/ADLSmount/bronze/movies_dataset/movies.csv', name='movies.csv', size=3112190, modificationTime=1755097078000)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/mnt/ADLSmount/bronze/movies_dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67385780-a55e-458a-b488-681f55eb5222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/ADLSsnipBronze.png</td><td>ADLSsnipBronze.png</td><td>151719</td><td>1755496848000</td></tr><tr><td>dbfs:/FileStore/tables/kaggle/</td><td>kaggle/</td><td>0</td><td>1755096332000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/ADLSsnipBronze.png",
         "ADLSsnipBronze.png",
         151719,
         1755496848000
        ],
        [
         "dbfs:/FileStore/tables/kaggle/",
         "kaggle/",
         0,
         1755096332000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uploaded the snip file to DBFS and re-checking its existence\n",
    "\n",
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e5965b-c423-4641-ac88-dce18326ce17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Bronze dataset is present in ADLS\n",
    "![Bronze Layer in ADLS](https://adb-582130891499017.17.azuredatabricks.net/files/tables/ADLSsnipBronze.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "891b1c1f-63bb-4311-b336-f8275479ef0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mount ADLS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}